{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm   # for progress bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# changing the directory \n",
    "os.chdir('../artifacts/02_09_2024_13_41_30\\data_ingestion/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data or images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the class name \n",
    "class_name = ['NORMAL','PNEUMONIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to  get the list of files\n",
    "def get_list_of_files(dir_name):\n",
    "    '''\n",
    "    input - The input directory location\n",
    "    output - Returns the list the files in the directory\n",
    "    '''\n",
    "    files_list = os.listdir(dir_name)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path+'/train/'+class_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_train = get_list_of_files(data_path+'/train/'+class_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_pneumonia_train = get_list_of_files(data_path+'/train/'+class_name[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_pneumonia_train = get_list_of_files(data_path+'/train/'+class_name[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_test = get_list_of_files(data_path+'/test/'+class_name[0])\n",
    "files_list_pneumonia_test = get_list_of_files(data_path+'/test/'+class_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_pneumonia_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train samples in Normal category {}\".format(len(files_list_normal_train)))\n",
    "print(\"Number of train samples in Pneumonia category {}\".format(len(files_list_pneumonia_train)))\n",
    "print(\"Number of test samples in Normal category {}\".format(len(files_list_normal_test)))\n",
    "print(\"Number of test samples in Pneumonia category {}\".format(len(files_list_pneumonia_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/train/NORMAL/'+ files_list_normal_train[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/train/PNEUMONIA/'+ files_list_pneumonia_train[np.random.randint(0,len(files_list_pneumonia_train))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_test))\n",
    "img = data_path + '/test/NORMAL/'+ files_list_normal_test[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/test/PNEUMONIA/'+ files_list_pneumonia_test[np.random.randint(0,len(files_list_pneumonia_test))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming the imgaes\n",
    "\n",
    "Now that we have seen the sample of the images let's transform the data now\n",
    "\n",
    "We need to perform transformation on both train and test images\n",
    "\n",
    "For Training data we need to perform the data augmentation also.\n",
    "\n",
    "Data Augmentation is done to create synthetic data.\n",
    "\n",
    "In Transformation we are doing Resize,CenterCrop,ColorJitter,RandomHorizontalFlip,RandomRotation,ToTensor and Normalize.\n",
    "\n",
    "Resize:- Resize the input image to the given size.\n",
    "\n",
    "CenterCrop:- Crops the given image at the center.\n",
    "\n",
    "ColorJitter:- Randomly change the brightness, contrast, saturation and hue of an image.\n",
    "\n",
    "RandomHorizontalFlip:- Horizontally flip the given image randomly with a given probability.\n",
    "\n",
    "RandomRotation:- Rotate the image by angle.\n",
    "\n",
    "ToTensor:- Convert numpy.ndarray to tensor.\n",
    "\n",
    "Normalize:- Normalize a float tensor image with mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loader\n",
    "\n",
    "For our usecase will be using the default data loader for Pytorch.\n",
    "\n",
    "We will be creating 2 data loaders one for the training data and other for the test data.\n",
    "\n",
    "batch size is a hyperparameter which we can tweak according to our need and system configuration.\n",
    "\n",
    "We can provide Image shuffling True for training data and False for test data.\n",
    "\n",
    "Pin memory is used to transfer the loaded dataset from CPU to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(os.path.join(data_path, 'train'), transform= train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(os.path.join(data_path, 'test'), transform= test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size= 2, shuffle= True, pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size= 2, shuffle= False, pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f'Number of train images: {len(train_data)}')\n",
    "print(f'Number of test images: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model Architecture\n",
    "\n",
    "\n",
    "First Layer is the input layer consisting of 3 input channels and output channels with kernel_size of 3X3, padding=0 and bias=True. \n",
    "\n",
    "The activation function we are using is ReLU and performing batch normalization.\n",
    "\n",
    "Then we are performing max pooling to extract the important features out of the image.\n",
    "\n",
    "Similarly we are passing our model through 9 convolutional layers.\n",
    "\n",
    "Finally we are passing out passing our model through a output layer in which we are getting binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creating custom CNN architecture for Image classification\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # Input Block\n",
    "        self.convolution_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "        self.pooling11 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convolution_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=20, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.pooling22 = nn.MaxPool2d(2, 2)\n",
    "        self.convolution_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10),\n",
    "        )\n",
    "        self.pooling33 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convolution_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.convolution_block5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "#         self.convblock6 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(10),\n",
    "#         )\n",
    "        self.convolution_block6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "#         self.convblock8 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(32)\n",
    "#         )\n",
    "        self.convolution_block7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "        self.convolution_block8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(14)\n",
    "        )\n",
    "        self.convolution_block9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        # OUTPUT BLOCK\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.convolution_block_out = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(4, 4), padding=0, bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_block1(x)\n",
    "        x = self.pooling11(x)\n",
    "        x = self.convolution_block2(x)\n",
    "        x = self.pooling22(x)\n",
    "        x = self.convolution_block3(x)\n",
    "        x = self.pooling33(x)\n",
    "        x = self.convolution_block4(x)\n",
    "        x = self.convolution_block5(x)\n",
    "#         x = self.convblock6(x)\n",
    "        x = self.convolution_block6(x)\n",
    "#         x = self.convblock8(x)\n",
    "        x = self.convolution_block7(x)\n",
    "        x = self.convolution_block8(x)\n",
    "        x = self.convolution_block9(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.convolution_block_out(x)\n",
    "        x = x.view(-1, 2)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check weather cuda is available in the system or not \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Available processor {}\".format(device))\n",
    "model = Net().to(device)\n",
    "# To check the model summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Description: To train the model \n",
    "    \n",
    "    input: model,device,train_loader,optimizer,epoch \n",
    "    \n",
    "    output: loss, batch id and accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Initialization of gradient\n",
    "        optimizer.zero_grad()\n",
    "        # In PyTorch, gradient is accumulated over backprop and even though thats used in RNN generally not used in CNN\n",
    "        # or specific requirements\n",
    "        ## prediction on data\n",
    "        y_pred = model(data)\n",
    "        # Calculating loss given the prediction\n",
    "        loss = F.nll_loss(y_pred, target)\n",
    "        train_losses.append(loss)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # get the index of the log-probability corresponding to the max value\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Description: To test the model\n",
    "    \n",
    "    input: model, device, test_loader\n",
    "    \n",
    "    output: average loss and accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the params for training \n",
    "model =  Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "scheduler = StepLR(optimizer, step_size=6, gamma=0.5)\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    print('current Learning Rate: ', optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses1 = [float(i.cpu().detach().numpy()) for i in train_losses]\n",
    "train_acc1 = [i for i in train_acc]\n",
    "test_losses1 = [i for i in test_losses]\n",
    "test_acc1 = [i for i in test_acc]\n",
    "fig, axs = plt.subplots(2,2,figsize=(16,10))\n",
    "axs[0, 0].plot(train_losses1)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[1, 0].plot(train_acc1)\n",
    "axs[1, 0].set_title(\"Training Accuracy\")\n",
    "axs[0, 1].plot(test_losses1)\n",
    "axs[0, 1].set_title(\"Test Loss\")\n",
    "axs[1, 1].plot(test_acc1)\n",
    "axs[1, 1].set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
